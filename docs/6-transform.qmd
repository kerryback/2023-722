---
title:  "Transforming Features"
author: <br><br><br><br><span style="font-family:perpetua; font-variant:small-caps; color:#404040;">Kerry Back</span><br><br><img src="RiceBusiness-transparent-logo-sm.png"  height=80>
execute:
  echo: false
  jupyter: python3
  cache: true
format: 
  revealjs:
    highlight-style: monokai
    code-fold: true
    scrollable: true
    slide-number: true
    preview-links: true
    self-contained: true
    controls: true
    transition: fade
    theme: [solarized, 721.scss]
    incremental: true
---

## Outliers, scaling, and polynomial features

- For neural nets and other methods, it is important to have predictors that are
  - on the same scale
  - free of outliers
- It is also useful to add squares and products of our predictors.
- We will (i) take care of outliers and scaling, (ii) add squares and products, and (iii) define a machine learning model all within a [pipeline]{style="color:tomato;"}.
  

## Neural net example of scaling and outliers

. . .

For a neuron with

. . .

$$ y = \max(0, b + w_1x_1 + \cdots + w_n x_n)$$

- to find the right $w$'s, it helps to have $x$'s of similar scales
- and outlier inputs can produce outlier outputs and large errors, so they may get excessive attention in fitting the model

## Quantile transformer

. . .

There are many ways to take care of outliers and scaling, but we'll just use one.

. . .

```p
from sklearn.preprocessing import QuantileTransformer

transform = QuantileTransformer(
    output_distribution="normal"
)
```


```{python}
from sqlalchemy import create_engine
import pymssql
import pandas as pd

server = "mssql-82792-0.cloudclusters.net:16272"
username = "user"
password = "RiceOwls1912" # paste password between quote marks
database = "ghz"

string = "mssql+pymssql://" + username + ":" + password + "@" + server + "/" + database

conn = create_engine(string).connect()

data = pd.read_sql(
    """
    select ticker, date, ret, roeq, mom12m
    from data
    where date='2021-12'
    """, 
    conn
)
data = data.dropna()
data['rnk'] = data.ret.rank(pct=True)

from sklearn.preprocessing import QuantileTransformer
transform = QuantileTransformer(
    output_distribution="normal"
)

old = data.roeq.to_numpy().reshape(-1,1)
new = transform.fit_transform(old)

from sklearn.neural_network import MLPRegressor

X = data[["roeq", "mom12m"]]
y = data["rnk"]

model = MLPRegressor(
  hidden_layer_sizes=(4, 2),
  random_state=0
)
```


## Example: roeq in 2021-12

. . .

Distribution before (old) and after (new)

. . .

::: {.panel-tabset}

## Box plots

:::: {.columns}

::: {.column width="50%"}


```{python}
import plotly.graph_objects as go
import numpy as np

old = np.ravel(old)
new = np.ravel(new)

trace1 = go.Box(
    y=old,
    name="old",
)

fig = go.Figure(trace1)

fig.update_layout(
    template="plotly_dark",
    xaxis_title_font_size=24,
    yaxis_title_font_size=24,
    font_size=20,
    yaxis_tickformat=".0%",
    width=480,
    height=420,
)

fig.show()
```
:::
::: {.column width="50%"}

```{python}
trace2 = go.Box(
    y=new,
    name="new",
)
fig = go.Figure(trace2)
fig.update_layout(
    template="plotly_dark",
    xaxis_title_font_size=24,
    yaxis_title_font_size=24,
    font_size=20,
    yaxis_tickformat=".0%",
    width=480,
    height=420,
)

fig.show()
```

:::
::::

## Density plots

:::: {.columns}
::: {.column width="50%"}

```{python}

from scipy.stats import gaussian_kde as kde

density = kde(old)
mn = np.min(old)
mx = np.max(old)
grid = np.linspace(mn, mx, 100)

trace = go.Scatter(
  x=grid, 
  y=density(grid),
  mode="lines", 
  name="old"
)
fig = go.Figure(trace)
fig.update_layout(
    template="plotly_dark",
    yaxis_title="",
    xaxis_title_font_size=24,
    yaxis_title_font_size=24,
    font_size=20,
    xaxis_tickformat=".0%",
    yaxis_tickformat="",
    width=480,
    height=420,
)

fig.show()
```

:::
::: {.column width="50%"}

```{python}
density = kde(new)
mn = np.min(new)
mx = np.max(new)
grid = np.linspace(mn, mx, 100)

trace = go.Scatter(
  x=grid, 
  y=density(grid),
  mode="lines", 
  name="new"
)
fig = go.Figure(trace)
fig.update_layout(
    template="plotly_dark",
    yaxis_title="",
    xaxis_title_font_size=24,
    yaxis_title_font_size=24,
    font_size=20,
    xaxis_tickformat=".0%",
    yaxis_tickformat="",
    width=480,
    height=420,
)

fig.show()
```

:::
::::
:::



## Pipelines

- This will be our process:
  - Apply quantile transformer
  - Add squares and products
  - Apply quantile transformer again
- We do this and define our ML model in a pipeline.
- Then we fit the pipeline and predict with it.


#


```{.p code-line-numbers="1-2|4|5-10|11"}
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

poly = PolynomialFeatures(degree=2)
pipe = make_pipeline(
  transform, 
  poly,
  transform,
  model
)
pipe.fit(X, y)
```

. . .

```{python}
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

poly = PolynomialFeatures(degree=2)
pipe = make_pipeline(
  transform, 
  poly,
  transform,
  model
)

from joblib import dump
pipe.fit(X, y)
_ = dump(pipe, "net2.joblib")
```

## Entire workflow: connect to database

. . .

```{.p}
from sqlalchemy import create_engine
import pymssql
import pandas as pd

server = "mssql-82792-0.cloudclusters.net:16272"
username = "user"
password = "" # paste password between quote marks
database = "ghz"

string = "mssql+pymssql://" + username + ":" + password + "@" + server + "/" + database

conn = create_engine(string).connect()
```

## Download data

. . .

```{.p}
data = pd.read_sql(
    """
    select ticker, date, ret, roeq, mom12m
    from data
    where date='2021-12'
    """, 
    conn
)
data = data.dropna()
data['rnk'] = data.ret.rank(pct=True)
```

## Import from scikit-learn

. . .

```{.p}
from sklearn.preprocessing import QuantileTransformer
from sklearn.preprocessing import PolynomialFeatures
from sklearn.neural_network import MLPRegressor
from sklearn.pipeline import make_pipeline
```

## Define pipeline

. . .

```p
transform = QuantileTransformer(
    output_distribution="normal"
)
poly = PolynomialFeatures(degree=2)
model = MLPRegressor(
  hidden_layer_sizes=(4, 2),
  random_state=0
)
pipe = make_pipeline(
  transform, 
  poly,
  transform,
  model
)
```

## Fit and save the pipeline

. . .

```{.p}
X = data[["roeq", "mom12m"]]
y = data["rnk"]

pipe.fit(X, y)

from joblib import dump, load
dump(pipe, "net2.joblib")
```

. . .

<br>
Later:

. . .

```p
net = load("net2.joblib")
```

## Comments

- Same workflow for random forest.  Just import RandomForestRegressor and use it in model = .
- We're going to change the last block in the next section.  Put the pipeline through GridSearchCV and fit it.

