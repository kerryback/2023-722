---
title:  "Cross Validation"
author: <br><br><br><br><span style="font-family:perpetua; font-variant:small-caps; color:#606060;">Kerry Back</span><br><br><img src="RiceBusiness-transparent-logo-sm.png"  height=120>
execute:
  echo: false
  jupyter: python3
  cache: true
format: 
  revealjs:
    highlight-style: monokai
    code-fold: true
    scrollable: true
    slide-number: true
    preview-links: true
    self-contained: true
    controls: true
    transition: fade
    theme: [white, 722.scss]
    incremental: true
---

## Ridge regression

- Consider linear model
$y = \beta_0 + \beta_1 x_1 + \cdots \beta_n x_n + \varepsilon$

- Usual (OLS) estimate of $\beta=(\beta_0, \ldots, \beta_n)$ minimizes SSE = sum of squared errors.

- Ridge regression minimizes SSE plus a penalty for large $\hat \beta$'s:

. . . 

$$\text{SSE} + \alpha \sum_{i=0}^n \hat \beta_i^2$$

- $\alpha$ is a hyperparameter.  Goal is to avoid overfitting. 


## Why the name "ridge"?

- Let  m = number of observations, $y$ = m-vector of dependent variable observations, $X$ = m $\times$ (n+1) matrix of independent variable observations, with first column being a column of 1's, $\beta$ = (n+1)-vector of coefficients.
- SSE = 

. . .

$$(y-X\hat \beta)^{\top} (y-X \hat\beta) $$

$$= y^{\top} y - 2 y^{\top } X\hat\beta + \hat\beta^{\top} X^{\top} X \hat\beta$$

#

$$\text{SSE} + \alpha \sum_{i=0}^n \hat \beta_i^2=$$

. . .

$$y^{\top} y - 2 y^{\top } X\hat\beta + \hat\beta^{\top} \big[X^{\top} X +\alpha I\big]\hat\beta$$

## New imports

. . .

```{.p}
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge
```

. . .

Create pipeline as before but use Ridge instead of LinearRegression

##

